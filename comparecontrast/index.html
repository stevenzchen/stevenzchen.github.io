<!DOCTYPE html>

<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />

  <!-- Page Title -->
  <title>Compare and Contrast: Learning Prominent Visual Differences</title>

  <link href="favicon.ico" rel="icon" type="image/x-icon" />
  <link href="styles/reset.css" rel="stylesheet" type="text/css" />
  <link href="styles/comparecontrast.css" rel="stylesheet" type="text/css" />
  <link href="styles/comparecontrast_mobile.css" rel="stylesheet" type="text/css" media="only screen and (max-device-width: 480px)" />
  <link href="styles/lightbox.css" rel="stylesheet" type="text/css" />

  <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
  <script src="styles/lightbox.min.js"></script>
    
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  
    ga('create', 'UA-75141465-1', 'auto');
    ga('send', 'pageview');
  
  </script>

  <script>
    $(document).ready(function(){
      $(".fakelink").click(function(){
        $(".bibref").slideToggle(600);
      });
    });
  </script>

</head>

<body>
  
  <div class="header">
    <h1>Compare and Contrast:<br> Learning Prominent Visual Differences</h1>
    <h2>Steven Chen&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kristen Grauman</h2>
    <h3><i>University of Texas at Austin</i></h3>
    <h4><span class="dl-list"><a onclick="ga('send', 'event', 'link', 'download', 'paper-main');" href="http://www.cs.utexas.edu/~grauman/papers/prominent_attributes_cvpr2018.pdf" target="_blank">[paper]</a>&nbsp;<a onclick="ga('send', 'event', 'link', 'download', 'paper-supp');" href="http://www.cs.utexas.edu/~grauman/papers/prominent-attributes-supp.pdf" target="_blank">[supp]</a>&nbsp;<a onclick="ga('send', 'event', 'link', 'download', 'paper-poster');" href="docs/prominent_differences_poster.pdf" target="_blank">[poster]</a>&nbsp;<a onclick="ga('send', 'event', 'link', 'download', 'paper-dataset');" href="docs/prominent_differences_dataset.zip" target="_blank">[dataset]</a>&nbsp;<a onclick="ga('send', 'event', 'link', 'download', 'paper-arxiv');" href="http://arxiv.org/abs/1804.00112" target="_blank">[arxiv]</a></span></h4>
  </div>
  
  
  <section class="abstract">
    
    <header><i>Abstract</i></header>

    <p><i>Relative attribute models can compare images in terms of all detected properties or attributes, exhaustively predicting which image is fancier, more natural, and so on without any regard to ordering. However, when humans compare images, certain differences will naturally stick out and come to mind first. These most noticeable differences, or prominent differences, are likely to be described first. In addition, many differences, although present, may not be mentioned at all. In this work, we introduce and model prominent differences, a rich new functionality for comparing images. We collect instance-level annotations of most noticeable differences, and build a model trained on relative attribute features that predicts prominent differences for unseen pairs. We test our model on the challenging UT-Zap50K shoes and LFW10 faces datasets, and outperform an array of baseline methods. We then demonstrate how our prominence model improves two vision tasks, image search and description generation, enabling more natural communication between people and vision systems.</i></p>
    
  </section>
  
  
  <section>
    
    <header>Main Idea</header>
    <hr class="midhr">

      <img class="width50" src="images/motivation.jpg" alt="Introduction" />

    <p>When people compare images, certain visual differences stick out, and are described first. These <i>prominent differences</i> are most noticeable, and are likely to be described first, while other differences, although present and true, are likely stated later or not at all. Prominent differences affect our perception of images (ex. what stands out when comparing shopping items?), and influence human input to vision systems (ex. labeling, feedback to an interactive system).</p>

    <p>In this work, we introduce and model prominent differences, a rich new functionality for comparing images. We collect annotations of most noticeable differences, and build a model trained on relative attributes that predicts prominent differences for unseen pairs. We evaluate on two challenging datasets and outperform an array of baseline methods. Finally, we demonstrate how prominent differences can be used to enhance two vision tasks, image search and description generation.</p>
    
  </section>
  
  <section>
    
    <header>Learning Prominent Differences</header>
    <hr class="midhr">

      <h2>Relative Attributes</h2>

      <p>Relative attributes <sup class="cite">[Parikh and Grauman 2011]</sup> express an image's attribute strength with respect to other images, and enable visual comparisons between images. Relative attribute rankers are models that can rank images based on their attribute strength scores, and use sets of labeled images pairs for learning.</p>

      <img class="width60" src="images/relattr.jpg" alt="Relative Attributes" />

    <p><b>Relative Attribute Examples: </b> Relative attributes such as <i>sporty</i> and <i>smiling</i> can rank images across a range of strengths.</p>

      <h2>Modeling</h2>

    <p>Given an image pair, we obtain the relative attribute scores for each image using a relative attribute ranker, and combine these scores into a symmetric, pairwise representation for the pair. We train a multiclass classifier on this feature representation using labeled prominent difference pairs. Given a new image pair, we use this trained model to predict prominent difference(s). See the paper for more detail.</p>

    <img class="width60" src="images/pipeline.jpg" alt="Learning Prominent Differences" />

      <p><b>Inference Pipeline: </b>Our approach pipeline for prominent difference prediction, as described above.</p>


  </section>
    
  <section>
    <header>Predicting Prominent Differences</header>
    <hr class="midhr">

      <p>We evaluate on two different image datasets, collecting prominent difference annotations for both:</p>

    <ul>  
      <li><b>UT-Zap50K <sup class="cite">[Yu and Grauman 2014]</sup>:</b> 50,025 shoe images, 10 relative attributes (sporty, formal, etc.), 5,000 prominence annotation pairs</li>
      <li><b>LFW-10 <sup class="cite">[Sandeep et al. 2014]</sup>:</b> 2,000 face images, 10 relative attributes (smiling, bald head, etc.), 5,000 prominence annotation pairs</li>
    </ul>

      <p>We benchmark prominence prediction accuracy compared to prior work <sup class="cite">[Turakhia and Parikh 2013 <b>**</b>]</sup> and several baseline approaches. We outperform prior work and all baselines, on both of our datasets and for both attribute rankers used.</p>
    
    <img class="width60" src="images/results.jpg" alt="Quantitative Results" />
    
    <p><b>Quantitative Results: </b>Prominence prediction accuracy results for the two datasets (top, bottom) and either SVM or CNN-based attribute rankers (left, right). Our approach outperforms all baselines on both datasets and for both rankers.</p>
    
    <img class="width75" src="images/imresults.jpg" alt="Qualitative Results" />
    
    <p><b>Qualitative Results: </b>Prominence predictions made by our approach. Predicted most prominent attribute in bold, followed by next two most confident attributes.</p>
    
  </section>
  
  <section>
    <header>Application: Description Generation</header>
    <hr class="midhr">

      <p>We apply prominent differences to generate textual image descriptions comparing images. When humans compare images, they state prominent differences first, and will not name all true differences for a pair. Previous work generates descriptions comparing images in terms of all attributes, in an arbitrary order. In our approach, we generate descriptions containing the most prominent differences for a pair. We demonstrate that our approach generates descriptions that are perceived as more natural and appropriate.</p>
                
    <img class="width50" src="images/descresults.jpg" alt="Description Results" />

      <img class="width40" src="images/desctable.jpg" alt="Description Results" />

      <p><b>Description Quantitative Results: </b> Offline description generation results on top, human study results on bottom. Our generated descriptions contain more prominent differences than other approaches. In the human study, judges perceive our generated descriptions as more natural and appropriate.</p>

      <img class="width50" src="images/descim.jpg" alt="Description Results" />

      <p><b>Description Qualitative Results: </b> Descriptions generated by our model highlight the prominent differences that stand out most between the images.</p>


  </section>

  <section>
    
    <header>Application: Image Search</header>
    <hr class="midhr">

      <p>We incorporate prominent differences into WhittleSearch <sup class="cite">[Kovashka et al. 12 <b>~</b>]</sup>, an interactive image search framework where users provide relative attribute feedback through comparisons (e.g., I would like images that are <i>more formal</i> than X). WhittleSearch ranks images by the number of feedback constraints satisfied. A problem with this apporach is that many images satisfy all feedback, and appear equally relevant to the system. When users provide relative feedback to the system, they are likely to provide prominent differences. Thus, in our approach, we order images by their prominence strength with user feedback. Our approach produces significantly more relevant results, yet requires no additional user feedback.

    <img class="width75" src="images/searchresults.jpg" alt="WhittleSearch Results" />

      <p><b>Image Search Results: </b> Quantitative results on left, displaying target image rank at each iteration (lower rank is better). On right, qualitative search results with user target images, followed by WhittleSearch top results and our results. Our approach produces more relevant search results using the same amount of user feedback.</p>

  </section>

  <section>
    
    <header>Dataset</header>
    <hr class="midhr">
    
    <p>Dataset is available for download <a href="docs/prominent_differences_dataset.zip">here</a>.</p>
    
  </section>
  
  
  <section>
    
    <header>Publication</header>
    <hr class="midhr">
    
    <p><span class="citation">S. Chen and K. Grauman. "Compare and Contrast: Learning Prominent Visual Differences". In CVPR, 2018.</span>&nbsp;<a class="fakelink">[bibtex]</a></p>
    
    <p class="bibref">@InProceedings{prominentdifferences,<br>&nbsp;&nbsp;author = {S. Chen and K. Grauman},<br>&nbsp;&nbsp;title = {Compare and Contrast: Learning Prominent Visual Differences},<br>&nbsp;&nbsp;booktitle = {CVPR},<br>&nbsp;&nbsp;month = {June},<br>&nbsp;&nbsp;year = {2018}<br>}</p>
    
  </section>
  
  <hr class="mainhr">
  <footer>Copyright &copy; 2018 University of Texas at Austin. Please contact <a onclick="ga('send', 'event', 'link', 'redirect', 'personal-home');" href="//stevenzc.com" target="_blank">Steven Chen</a> for comments and questions.</footer>
  
</body>

</html>
